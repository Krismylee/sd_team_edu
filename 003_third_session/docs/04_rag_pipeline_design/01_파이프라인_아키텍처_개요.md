# 01. 파이프라인 아키텍처 개요

## 이 챕터에서 배우는 것

- RAG 파이프라인의 전체 구조
- 단계별 역할과 입력/출력 계약
- 설계 시 흔한 실패 지점

---

## 1. 전체 파이프라인 흐름

검색 → 후처리 → 결합 → 후처리 → 생성

각 단계는 **입력/출력 계약**을 명확히 해야 안정적으로 동작합니다.

---

## 2. 단계별 역할

- **검색**: 후보 문서 수집
- **후처리 1**: 노이즈 제거 및 재정렬
- **결합**: 다중 검색 결과 병합
- **후처리 2**: 최종 컨텍스트 압축
- **생성**: 답변 작성 및 포맷팅

---

## 3. 데이터 계약 예시

- 검색 출력: `doc_id`, `score`, `content`, `metadata`
- 후처리 출력: `doc_id`, `rank`, `content`
- 생성 입력: `question`, `contexts`, `constraints`

---

## 4. 흔한 실패 지점

- 단계 간 **스키마 불일치**
- 후처리 기준이 모호해 **품질 편차** 발생
- 생성 단계에 **불필요한 문맥**이 과다 투입

---

## 5. 성능 개선을 위한 상위 레벨 최적화(개요)

파이프라인 관점에서 성능 최적화는 **단계별 병목을 제거**하는 작업입니다.
검색만 빠르게 만드는 것이 아니라, **수집/임베딩/저장/검색/후처리** 전체를 함께 봐야 합니다.

### 1) 임베딩 생성 최적화

- **배치 임베딩**으로 네트워크 오버헤드 감소
- **비동기/큐 기반 처리**로 대량 입력 지연 완화
- **중복 제거 캐시**로 동일 문서 임베딩 재사용

### 2) 저장 최적화

- **배치 INSERT**(executemany)로 저장 비용 절감
- **커넥션 풀**로 연결 오버헤드 제거
- **업서트/버전 관리**로 재색인 비용 최소화

### 3) 검색 최적화

- **사전 필터링**으로 후보 수 축소
- **근사 탐색 파라미터** 튜닝으로 지연 시간 조절
- **Top-K 제한**으로 후처리 비용 제어

### 4) 후처리 최적화

- **중복 제거 기준 고정**으로 일관성 확보
- **재정렬 범위 제한**으로 비용 절감

### 5) 운영/관측 최적화

- 검색 지연 시간, 실패율, 비용을 **지표로 고정**
- 파라미터 변경 이력과 성능 지표를 함께 관리

---

## 6. 최적화된 구현 예시(상위 레벨, 개념)

아래 코드는 **배치 임베딩 + 배치 저장 + 커넥션 풀**을 포함한
상위 레벨 최적화 흐름을 보여줍니다.

> 실서비스에서는 큐/워커/스케줄러와 함께 운영하는 것이 일반적입니다.

```python
"""
목적: 배치 임베딩과 배치 저장으로 성능을 개선한다.
설명: 비동기 임베딩 + 커넥션 풀 + executemany 흐름을 보여준다.
디자인 패턴: Pipeline, Repository
"""

from dataclasses import dataclass
from typing import Sequence
import asyncio
import os
import requests
from psycopg_pool import ConnectionPool


@dataclass(frozen=True)
class ChunkRecord:
    """청크 저장 레코드."""

    content: str
    embedding: Sequence[float]
    metadata: dict


class AsyncEmbeddingService:
    """비동기 임베딩 생성 서비스."""

    def __init__(self, model: str) -> None:
        self._model = model

    async def embed_batch(self, texts: Sequence[str]) -> list[list[float]]:
        """텍스트 목록을 한 번에 임베딩한다."""
        url = "https://api.openai.com/v1/embeddings"
        headers = {
            "Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}",
            "Content-Type": "application/json",
        }
        payload = {"model": self._model, "input": list(texts)}
        response = await asyncio.to_thread(
            requests.post, url, headers=headers, json=payload, timeout=30
        )
        response.raise_for_status()
        return [item["embedding"] for item in response.json()["data"]]


class BatchRepository:
    """배치 저장을 담당한다."""

    def __init__(self, dsn: str, table_name: str = "documents") -> None:
        self._table_name = table_name
        self._pool = ConnectionPool(conninfo=dsn, min_size=1, max_size=5)

    def save_many(self, records: list[ChunkRecord]) -> None:
        """여러 레코드를 한 번에 저장한다."""
        sql = f"""
        INSERT INTO {self._table_name} (content, embedding, metadata)
        VALUES (%s, %s, %s)
        """
        params = [(r.content, r.embedding, r.metadata) for r in records]
        with self._pool.connection() as conn:
            conn.executemany(sql, params)


async def ingest_batches(
    texts: list[str],
    metas: list[dict],
    embedder: AsyncEmbeddingService,
    repo: BatchRepository,
    batch_size: int = 64,
) -> None:
    """배치 단위로 임베딩 후 저장한다."""
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i : i + batch_size]
        batch_metas = metas[i : i + batch_size]
        vectors = await embedder.embed_batch(batch_texts)
        records = [
            ChunkRecord(content=t, embedding=v, metadata=m)
            for t, v, m in zip(batch_texts, vectors, batch_metas)
        ]
        repo.save_many(records)
```

**적용 기준**

- 입력이 커질수록 **배치 크기와 동시성**을 먼저 튜닝
- DB 병목이 크면 **저장 배치를 줄이고** 커넥션 풀을 늘림
- API 병목이 크면 **배치 크기를 늘리고** 호출 횟수를 줄임

---

## 7. 체크리스트

- 각 단계의 역할을 명확히 정의했는가?
- 입력/출력 스키마를 문서화했는가?
- 실패 지점을 예상하고 대비했는가?
